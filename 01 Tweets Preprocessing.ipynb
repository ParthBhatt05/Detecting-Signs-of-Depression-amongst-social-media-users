{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "import datetime\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from wordsegment import load, segment\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "import tqdm\n",
    "\n",
    "load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Load the depression lexicon to seed the LDA topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading Depression PHQ-9 Lexicon\n",
    "with open(\"Data/depression_lexicon.json\") as f:\n",
    "    seed_terms = json.load(f)\n",
    "\n",
    "# read all seed terms into a list removing the underscore from all seeds\n",
    "seed_terms_col = [\n",
    "    seed.replace(\"_\", \" \") for seed in list(\n",
    "        itertools.chain.from_iterable(\n",
    "            [seed_terms[signal] for signal in seed_terms.keys()]))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Prepare other lexicons and resources required to filter and pre-process the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other lexicons and resources\n",
    "emojies = [\":‑)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\", \":^)\", \":っ)\", \":‑D\", \"8‑D\", \"8D\", \"x‑D\", \"xD\", \"X‑D\", \"XD\", \"=‑D\", \"=D\", \"=‑3\", \"=3\", \"B^D\", \":-))\", \">:[\", \":‑(\", \":(\", \":‑c\", \":c\", \":‑<\", \":っC\", \":<\", \":‑[\", \":[\", \":{\", \";(\", \":-||\", \":@\", \">:(\", \":'‑(\", \":'(\", \":'‑)\", \":')\", \"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D‑':\", \">:O\", \":‑O\", \":O\", \":‑o\", \":o\", \"8‑0\", \"O_O\", \"o‑o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\", \":*\", \":-*\", \":^*\", \"(\", \"}{'\", \")\", \";‑)\", \";)\", \"*-)\", \"*)\", \";‑]\", \";]\", \";D\", \";^)\", \":‑,\", \">:P\", \":‑P\", \":P\", \"X‑P\", \"x‑p\",\n",
    "           \"xp\", \"XP\", \":‑p\", \":p\", \"=p\", \":‑Þ\", \":Þ\", \":þ\", \":‑þ\", \":‑b\", \":b\", \"d:\", \">:\\\\\", \">:/\", \":‑/\", \":‑.\", \":/\", \":\\\\\", \"=/\", \"=\\\\\", \":L\", \"=L\", \":S\", \">.<\", \":|\", \":‑|\", \":$\", \":‑X\", \":X\", \":‑#\", \":#\", \"O:‑)\", \"0:‑3\", \"0:3\", \"0:‑)\", \"0:)\", \"0;^)\", \">:)\", \">;)\", \">:‑)\", \"}:‑)\", \"}:)\", \"3:‑)\", \"3:)\", \"o/\\o\", \"^5\", \">_>^\", \"^<_<\", \"|;‑)\", \"|‑O\", \":‑J\", \":‑&\", \":&\", \"#‑)\", \"%‑)\", \"%)\", \":‑###..\", \":###..\", \"<:‑|\", \"<*)))‑{\", \"><(((*>\", \"><>\", \"\\o/\", \"*\\0/*\", \"@}‑;‑'‑‑‑\", \"@>‑‑>‑‑\", \"~(_8^(I)\", \"5:‑)\", \"~:‑\\\\\", \"//0‑0\\\\\\\\\", \"*<|:‑)\", \"=:o]\", \"7:^]\", \",:‑)\", \"</3\", \"<3\"]\n",
    "\n",
    "\n",
    "tweet_token = TweetTokenizer(\n",
    "    preserve_case=True, reduce_len=True, strip_handles=True)\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "punctuation.remove(\"-\")\n",
    "punctuation.remove('_')\n",
    "\n",
    "stop_words_extended = [\"a's\", \"abaft\", \"able\", \"aboard\", \"above\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afore\", \"aforesaid\", \"afterwards\", \"against\", \"agin\", \"ago\", \"ah\", \"ain't\", \"aint\", \"albeit\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"alongside\", \"already\", \"also\", \"although\", \"always\", \"american\", \"amid\", \"amidst\", \"among\", \"amongst\", \"and\", \"anent\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"aren\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"aslant\", \"associated\", \"astride\", \"athwart\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"bar\", \"barring\", \"became\", \"become\", \"becomes\", \"becoming\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beneath\", \"beside\", \"besides\", \"best\", \"better\", \"betwixt\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"by\", \"c\", \"c'mon\", \"c's\", \"ca\", \"came\", \"can't\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\", \"circa\", \"clearly\", \"close\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"cos\", \"could\", \"couldn't\", \"couldnt\", \"couldst\", \"course\", \"currently\", \"dare\", \"dared\", \"daren\", \"dares\", \"daring\", \"date\", \"definitely\", \"described\", \"despite\", \"didn\", \"different\", \"directly\", \"does\", \"doesn't\", \"don\", \"done\", \"dost\", \"doth\", \"downwards\", \"due\", \"durst\", \"e\", \"early\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"em\", \"end\", \"ending\", \"english\", \"enough\", \"entirely\", \"er\", \"ere\", \"especially\", \"et\", \"et-al\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"excepting\", \"f\", \"failing\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"further\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"going\", \"gone\", \"gonna\", \"got\", \"gotta\", \"gotten\", \"greetings\", \"h\", \"hadn\", \"happens\", \"hard\", \"hardly\", \"hasn\", \"hast\", \"hath\", \"haven\", \"having\", \"he'd\", \"he'll\", \"he's\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"here's\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"herself\", \"hes\", \"hi\", \"hid\", \"high\", \"himself\", \"hither\", \"home\", \"hopefully\", \"how's\", \"howbeit\", \"however\", \"hundred\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"id\", \"ie\", \"ignored\", \"ill\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"inside\", \"insofar\", \"instantly\", \"instead\", \"invention\", \"inward\", \"isn\", \"it\", \"it'd\", \"it'll\", \"itd\", \"itself\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"large\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"left\", \"less\", \"lest\", \"let\", \"let's\", \"lets\", \"like\", \"liked\", \"likely\", \"likewise\", \"line\", \"little\", \"living\", \"long\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mayn\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"mid\", \"midst\", \"might\", \"million\", \"mine\", \"minus\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"mustn't\", \"myself\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"neath\", \"necessarily\", \"necessary\", \"need\", \"needed\", \"needing\", \"needs\", \"neither\", \"never\", \"nevertheless\",\n",
    "                       \"new\", \"next\", \"nigh\", \"nigher\", \"nighest\", \"nine\", \"ninety\", \"nisi\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"notwithstanding\", \"novel\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"once\", \"one\", \"ones\", \"oneself\", \"onto\", \"open\", \"ord\", \"others\", \"otherwise\", \"ought\", \"oughtn\", \"ours\", \"out\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"pending\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provided\", \"provides\", \"providing\", \"public\", \"put\", \"q\", \"qua\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"real\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respecting\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"round\", \"run\", \"said\", \"sans\", \"save\", \"saving\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"shalt\", \"shan't\", \"she'd\", \"she'll\", \"shed\", \"shell\", \"shes\", \"short\", \"shouldn\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"small\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"special\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"summat\", \"sup\", \"supposing\", \"sure\", \"t's\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"that'll\", \"that's\", \"that've\", \"thats\", \"thee\", \"theirs\", \"themselves\", \"thence\", \"there'll\", \"there's\", \"there've\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"theyd\", \"theyre\", \"thine\", \"think\", \"third\", \"tho\", \"thorough\", \"thoroughly\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"thro\", \"throug\", \"throughout\", \"thru\", \"thus\", \"thyself\", \"til\", \"till\", \"tip\", \"today\", \"together\", \"took\", \"touching\", \"toward\", \"towards\", \"tried\", \"tries\", \"true\", \"truly\", \"try\", \"trying\", \"ts\", \"twas\", \"tween\", \"twere\", \"twice\", \"twill\", \"twixt\", \"two\", \"twould\", \"u\", \"un\", \"underneath\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"versus\", \"via\", \"vice\", \"vis-a-vis\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"wanna\", \"want\", \"wanting\", \"wants\", \"wasn\", \"wasnt\", \"way\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"wed\", \"welcome\", \"well\", \"went\", \"weren\", \"werent\", \"wert\", \"what'll\", \"what's\", \"whatever\", \"whats\", \"when's\", \"whence\", \"whencesoever\", \"whenever\", \"where's\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whichever\", \"whichsoever\", \"whilst\", \"whim\", \"whither\", \"who'll\", \"who's\", \"whod\", \"whoever\", \"whole\", \"whomever\", \"whore\", \"whos\", \"whose\", \"whoso\", \"whosoever\", \"why's\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wonder\", \"wont\", \"words\", \"world\", \"would\", \"wouldn't\", \"wouldnt\", \"wouldst\", \"www\", \"x\", \"ye\", \"yes\", \"yet\", \"you'd\", \"you're\", \"youd\", \"youre\", \"yourself\", \"z\", \"zero\"]\n",
    "stop_words_extended = list(\n",
    "    set(stop_words_extended + punctuation + list(stopwords.words('english'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "##### Load the 1.6M tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('Data/tweets.csv', encoding=\"ISO-8859-1\",\n",
    "                        names=[\"sentiment\", \"tweet_id\", \"created_at\", \"query\", \"username\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date):\n",
    "    return datetime.strptime(date.replace(' PDT', ''), \"%a %b %d %H:%M:%S %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['created_at'] = tweets_df['created_at'].apply(convert_date)\n",
    "\n",
    "tweets_df = tweets_df.sort_values(\n",
    "    [\"username\", \"created_at\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tweet_counts=tweets_df[['tweet_id', 'username', 'created_at']].groupby(['username']).agg('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_50 = list(user_tweet_counts['username'][user_tweet_counts['tweet_id']>=50])\n",
    "users_70 = list(user_tweet_counts['username'][user_tweet_counts['tweet_id']>=70])\n",
    "users_100 = list(user_tweet_counts['username'][user_tweet_counts['tweet_id']>=100])\n",
    "users_180 = list(user_tweet_counts['username'][user_tweet_counts['tweet_id']>=180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tweets_50(username):\n",
    "    if username in users_50:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def user_tweets_70(username):\n",
    "    if username in users_70:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def user_tweets_100(username):\n",
    "    if username in users_100:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def user_tweets_180(username):\n",
    "    if username in users_180:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['_50'] = tweets_df['username'].apply(user_tweets_50)\n",
    "tweets_df['_70'] = tweets_df['username'].apply(user_tweets_70)\n",
    "tweets_df['_100'] = tweets_df['username'].apply(user_tweets_100)\n",
    "tweets_df['_180'] = tweets_df['username'].apply(user_tweets_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df=tweets_df.drop_duplicates()\n",
    "tweets_df_180=tweets_df[tweets_df['_180']==1]\n",
    "tweets_df_100=tweets_df[tweets_df['_100']==1]\n",
    "tweets_df_70=tweets_df[tweets_df['_70']==1]\n",
    "tweets_df_50=tweets_df[tweets_df['_50']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Pre-process tweets by filtering the text and recording the sentiments of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def deEmojify(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               u\"\\U00002500-\\U00002BEF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def de_abbreviate(token):\n",
    "    \"\"\"Convert the common abbreviations and misspellings to their formal and correct form\"\"\"\n",
    "\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow' or token == '2moro':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token in ['hahah', 'hahaha', 'hahahaha']:\n",
    "        return 'haha'\n",
    "    if token == \"mother's\":\n",
    "        return \"mother\"\n",
    "    if token == \"mom's\":\n",
    "        return \"mom\"\n",
    "    if token == \"dad's\":\n",
    "        return \"dad\"\n",
    "    if token == 'bday' or token == 'b-day':\n",
    "        return 'birthday'\n",
    "    if token in [\"i'm\", \"don't\", \"can't\", \"couldn't\", \"aren't\", \"wouldn't\", \"isn't\", \"didn't\", \"hadn't\",\n",
    "                 \"doesn't\", \"won't\", \"haven't\", \"wasn't\", \"hasn't\", \"shouldn't\", \"ain't\", \"they've\"]:\n",
    "        return token.replace(\"'\", \"\")\n",
    "    if token in ['lmao', 'lolz', 'rofl']:\n",
    "        return 'lol'\n",
    "    if token == '<3':\n",
    "        return 'love'\n",
    "    if token == 'thanx' or token == 'thnx':\n",
    "        return 'thanks'\n",
    "    if token == 'goood':\n",
    "        return 'good'\n",
    "    if token in ['amp', 'quot', 'lt', 'gt', '½25', '..', '. .', '. . .', '...']:\n",
    "        return ' '\n",
    "\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "\n",
    "def de_slang(tweet):\n",
    "\n",
    "    tweet = tweet.replace(\"idk\", \"i dont know\")\n",
    "    tweet = tweet.replace(\"i'll\", \"i will\")\n",
    "    tweet = tweet.replace(\"you'll\", \"you will\")\n",
    "    tweet = tweet.replace(\"we'll\", \"we will\")\n",
    "    tweet = tweet.replace(\"it'll\", \"it will\")\n",
    "    tweet = tweet.replace(\"it's\", \"it is\")\n",
    "    tweet = tweet.replace(\"i've\", \"i have\")\n",
    "    tweet = tweet.replace(\"you've\", \"you have\")\n",
    "    tweet = tweet.replace(\"we've\", \"we have\")\n",
    "    tweet = tweet.replace(\"they've\", \"they have\")\n",
    "    tweet = tweet.replace(\"you're\", \"you are\")\n",
    "    tweet = tweet.replace(\"we're\", \"we are\")\n",
    "    tweet = tweet.replace(\"they're\", \"they are\")\n",
    "    tweet = tweet.replace(\"let's\", \"let us\")\n",
    "    tweet = tweet.replace(\"she's\", \"she is\")\n",
    "    tweet = tweet.replace(\"he's\", \"he is\")\n",
    "    tweet = tweet.replace(\"that's\", \"that is\")\n",
    "    tweet = tweet.replace(\"i'd\", \"i would\")\n",
    "    tweet = tweet.replace(\"you'd\", \"you would\")\n",
    "    tweet = tweet.replace(\"there's\", \"there is\")\n",
    "    tweet = tweet.replace(\"what's\", \"what is\")\n",
    "    tweet = tweet.replace(\"how's\", \"how is\")\n",
    "    tweet = tweet.replace(\"who's\", \"who is\")\n",
    "    tweet = tweet.replace(\"y'all\", \"you all\")\n",
    "    tweet = tweet.replace(\"ya'll\", \"you all\")\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_text(tweet):\n",
    "\n",
    "    # replace seeds (as phrases) to unigrams.\n",
    "    for seed in seed_terms_col:\n",
    "        if seed in tweet and \" \" in seed:\n",
    "            tweet = tweet.replace(seed, seed.replace(\" \", \"_\"))\n",
    "\n",
    "    # remove retweet handler\n",
    "    if tweet[:2] == \"RT\":\n",
    "        try:\n",
    "            tweet = tweet[tweet.index(\":\") + 2:]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # remove url from tweet\n",
    "    tweet = re.sub(\n",
    "        r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "\n",
    "    tweet = de_slang(tweet)\n",
    "\n",
    "    # remove non-ascii characters\n",
    "    tweet = ''.join((filter(lambda x: x in printable, tweet)))\n",
    "\n",
    "    # additional preprocessing\n",
    "    tweet = tweet.replace(\"\\n\", \" \").replace(\" https\", \"\").replace(\"http\", \"\")\n",
    "\n",
    "    # remove all mentions\n",
    "    mentions = re.findall(r\"@\\w+\", tweet)\n",
    "    for mention in mentions:\n",
    "        tweet = tweet.replace(mention, \"\")\n",
    "\n",
    "    # clean usernames and hashtags\n",
    "    for term in re.findall(r\"#\\w+\", tweet):\n",
    "\n",
    "        # remove any punctuations from the hashtag and mention\n",
    "        token = term[1:].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        segments = ' '.join(segment(token))\n",
    "\n",
    "        tweet = tweet.replace(term, segments)\n",
    "\n",
    "    # remove all punctuations\n",
    "    tweet = re.sub(r\"\"\"\n",
    "               [\"\"\"+\"\".join(punctuation)+\"\"\"]+\n",
    "               \"\"\",\n",
    "                   \" \",\n",
    "                   tweet, flags=re.VERBOSE)\n",
    "\n",
    "    # remove trailing spaces\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r'[\\d-]+', 'NUM', tweet)\n",
    "\n",
    "    # pad NUM with spaces\n",
    "    tweet = tweet.replace(\"NUM\", \" NUM \")\n",
    "\n",
    "    # remove emojis\n",
    "    tweet = deEmojify(tweet)\n",
    "\n",
    "    # remove all stop words or emojis\n",
    "    tweet = \" \".join([de_abbreviate(word.lower()) for word in tweet_token.tokenize(tweet) if word.lower(\n",
    "    ) not in stop_words_extended and word.lower() not in emojies and len(word) > 1])\n",
    "\n",
    "    # remove multiple spaces\n",
    "    tweet = re.sub(' +', ' ', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess(tweets):\n",
    "\n",
    "    processed_tweets = []\n",
    "\n",
    "    for index, tweet in tqdm.tqdm(tweets.iterrows()):\n",
    "        cleaned_text = preprocess_text(tweet['text'])\n",
    "        sent_score = TextBlob(tweet['text']).sentiment.polarity\n",
    "        vader_compound_score = analyzer.polarity_scores(tweet['text'])[\n",
    "            'compound']\n",
    "        vader_positive_score = analyzer.polarity_scores(tweet['text'])['pos']\n",
    "        vader_negative_score = analyzer.polarity_scores(tweet['text'])['neg']\n",
    "        vader_neutral_score = analyzer.polarity_scores(tweet['text'])['neu']\n",
    "        sent_score_2 = TextBlob(cleaned_text).sentiment.polarity\n",
    "        vader_compound_score_2 = analyzer.polarity_scores(cleaned_text)[\n",
    "            'compound']\n",
    "        vader_positive_score_2 = analyzer.polarity_scores(cleaned_text)['pos']\n",
    "        vader_negative_score_2 = analyzer.polarity_scores(cleaned_text)['neg']\n",
    "        vader_neutral_score_2 = analyzer.polarity_scores(cleaned_text)['neu']\n",
    "\n",
    "        processed_tweets.append([tweet['tweet_id'], tweet['created_at'], tweet['text'], cleaned_text, sent_score, vader_compound_score, vader_positive_score,\n",
    "                                vader_neutral_score, vader_negative_score, sent_score_2, vader_compound_score_2, vader_positive_score_2, vader_neutral_score_2, vader_negative_score_2])\n",
    "\n",
    "    return pd.DataFrame(processed_tweets, columns=['tweet_id', 'created_at', 'text', 'cleaned_text', 'polarity_raw', 'vader_compound_raw', 'vader_pos_raw',\n",
    "                                                   'vader_neu_raw', 'vader_neg_raw', 'polarity_cleaned', 'vader_compound_cleaned', 'vader_pos_cleaned', 'vader_neu_cleaned', 'vader_neg_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweets = preprocess(tweets_df[[\"tweet_id\", \"created_at\", \"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweets=pd.merge(preprocessed_tweets, tweets_df[[\"tweet_id\",\"created_at\",\"username\",\"_50\",\"_70\", \"_100\"]], on=[\"tweet_id\",'created_at'])\n",
    "preprocessed_tweets=preprocessed_tweets.drop_duplicates()\n",
    "preprocessed_tweets = preprocessed_tweets.sort_values([\"username\", \"created_at\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweets.to_csv('Data/tweets_cleaned.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
