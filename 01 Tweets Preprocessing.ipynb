{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:07:22.300458Z",
     "start_time": "2021-08-18T13:07:17.858767Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "import datetime\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from wordsegment import load, segment\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "import tqdm\n",
    "\n",
    "load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Load the depression lexicon to seed the LDA topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:12:43.350487Z",
     "start_time": "2021-08-18T13:12:43.293129Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading Depression PHQ-9 Lexicon\n",
    "with open(\"depression_lexicon.json\") as f:\n",
    "    seed_terms = json.load(f)\n",
    "\n",
    "# read all seed terms into a list removing the underscore from all seeds\n",
    "seed_terms_col = [\n",
    "    seed.replace(\"_\", \" \") for seed in list(\n",
    "        itertools.chain.from_iterable(\n",
    "            [seed_terms[signal] for signal in seed_terms.keys()]))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Prepare other lexicons and resources required to filter and pre-process the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:12:47.078180Z",
     "start_time": "2021-08-18T13:12:46.998730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Other lexicons and resources\n",
    "emojies = [\":‑)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\", \":^)\", \":っ)\",\n",
    "           \":‑D\", \"8‑D\", \"8D\", \"x‑D\", \"xD\", \"X‑D\", \"XD\", \"=‑D\", \"=D\", \"=‑3\", \"=3\", \"B^D\", \":-))\", \">:[\",\n",
    "           \":‑(\", \":(\", \":‑c\", \":c\", \":‑<\", \":っC\", \":<\", \":‑[\", \":[\", \":{\", \";(\", \":-||\", \":@\", \">:(\",\n",
    "           \":'‑(\", \":'(\", \":'‑)\", \":')\", \"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D‑':\", \">:O\", \":‑O\",\n",
    "           \":O\", \":‑o\", \":o\", \"8‑0\", \"O_O\", \"o‑o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\", \":*\", \":-*\", \":^*\", \"(\", \"}{'\",\n",
    "           \")\", \";‑)\", \";)\", \"*-)\", \"*)\", \";‑]\", \";]\", \";D\", \";^)\", \":‑,\", \">:P\", \":‑P\", \":P\", \"X‑P\", \"x‑p\",\n",
    "           \"xp\", \"XP\", \":‑p\", \":p\", \"=p\", \":‑Þ\", \":Þ\", \":þ\", \":‑þ\", \":‑b\", \":b\", \"d:\", \">:\\\\\", \">:/\", \":‑/\",\n",
    "           \":‑.\", \":/\", \":\\\\\", \"=/\", \"=\\\\\", \":L\", \"=L\", \":S\", \">.<\", \":|\", \":‑|\", \":$\", \":‑X\", \":X\", \":‑#\",\n",
    "           \":#\", \"O:‑)\", \"0:‑3\", \"0:3\", \"0:‑)\", \"0:)\", \"0;^)\", \">:)\", \">;)\", \">:‑)\", \"}:‑)\", \"}:)\", \"3:‑)\",\n",
    "           \"3:)\", \"o/\\o\", \"^5\", \">_>^\", \"^<_<\", \"|;‑)\", \"|‑O\", \":‑J\", \":‑&\", \":&\", \"#‑)\", \"%‑)\", \"%)\",\n",
    "           \":‑###..\", \":###..\", \"<:‑|\", \"<*)))‑{\", \"><(((*>\", \"><>\", \"\\o/\", \"*\\0/*\", \"@}‑;‑'‑‑‑\", \"@>‑‑>‑‑\",\n",
    "           \"~(_8^(I)\", \"5:‑)\", \"~:‑\\\\\", \"//0‑0\\\\\\\\\", \"*<|:‑)\", \"=:o]\", \"7:^]\", \",:‑)\", \"</3\", \"<3\"]\n",
    "\n",
    "\n",
    "tweet_token = TweetTokenizer(\n",
    "    preserve_case=True, reduce_len=True, strip_handles=True)\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "punctuation.remove(\"-\")\n",
    "punctuation.remove('_')\n",
    "\n",
    "stop_words_extended = [\n",
    "    \"a's\", \"abaft\", \"able\", \"aboard\", \"above\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\",\n",
    "    \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afore\", \"aforesaid\", \"afterwards\", \"against\", \"agin\", \"ago\", \"ah\",\n",
    "    \"ain't\", \"aint\", \"albeit\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"alongside\", \"already\", \"also\", \"although\",\n",
    "    \"always\", \"american\", \"amid\", \"amidst\", \"among\", \"amongst\", \"and\", \"anent\", \"announce\", \"another\", \"anybody\", \"anyhow\",\n",
    "    \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"apparently\", \"appear\", \"appreciate\",\n",
    "    \"appropriate\", \"approximately\", \"aren\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"aslant\", \"associated\",\n",
    "    \"astride\", \"athwart\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"bar\", \"barring\", \"became\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\",\n",
    "    \"beneath\", \"beside\", \"besides\", \"best\", \"better\", \"betwixt\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"by\", \"c\", \"c'mon\",\n",
    "    \"c's\", \"ca\", \"came\", \"can't\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\", \"circa\", \"clearly\",\n",
    "    \"close\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\",\n",
    "    \"contains\", \"corresponding\", \"cos\", \"could\", \"couldn't\", \"couldnt\", \"couldst\", \"course\", \"currently\", \"dare\", \"dared\",\n",
    "    \"daren\", \"dares\", \"daring\", \"date\", \"definitely\", \"described\", \"despite\", \"didn\", \"different\", \"directly\", \"does\",\n",
    "    \"doesn't\", \"don\", \"done\", \"dost\", \"doth\", \"downwards\", \"due\", \"durst\", \"e\", \"early\", \"ed\", \"edu\", \"effect\", \"eg\",\n",
    "    \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"em\", \"end\", \"ending\", \"english\", \"enough\", \"entirely\", \"er\",\n",
    "    \"ere\", \"especially\", \"et\", \"et-al\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\",\n",
    "    \"ex\", \"exactly\", \"example\", \"except\", \"excepting\", \"f\", \"failing\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\",\n",
    "    \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"further\", \"furthermore\", \"g\",\n",
    "    \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"going\", \"gone\", \"gonna\", \"got\",\n",
    "    \"gotta\", \"gotten\", \"greetings\", \"h\", \"hadn\", \"happens\", \"hard\", \"hardly\", \"hasn\", \"hast\", \"hath\", \"haven\", \"having\",\n",
    "    \"he'd\", \"he'll\", \"he's\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"here's\", \"hereafter\", \"hereby\", \"herein\", \"heres\",\n",
    "    \"hereupon\", \"herself\", \"hes\", \"hi\", \"hid\", \"high\", \"himself\", \"hither\", \"home\", \"hopefully\", \"how's\", \"howbeit\",\n",
    "    \"however\", \"hundred\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"id\", \"ie\", \"ignored\", \"ill\", \"im\", \"immediate\", \"immediately\",\n",
    "    \"importance\", \"important\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\",\n",
    "    \"inner\", \"inside\", \"insofar\", \"instantly\", \"instead\", \"invention\", \"inward\", \"isn\", \"it\", \"it'd\", \"it'll\", \"itd\",\n",
    "    \"itself\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"large\", \"largely\", \"last\",\n",
    "    \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"left\", \"less\", \"lest\", \"let\", \"let's\", \"lets\", \"like\", \"liked\",\n",
    "    \"likely\", \"likewise\", \"line\", \"little\", \"living\", \"long\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\",\n",
    "    \"makes\", \"many\", \"may\", \"maybe\", \"mayn\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"mid\", \"midst\",\n",
    "    \"might\", \"million\", \"mine\", \"minus\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"mustn't\",\n",
    "    \"myself\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"neath\", \"necessarily\", \"necessary\", \"need\",\n",
    "    \"needed\", \"needing\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nigh\", \"nigher\", \"nighest\", \"nine\",\n",
    "    \"ninety\", \"nisi\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\",\n",
    "    \"notwithstanding\", \"novel\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\",\n",
    "    \"omitted\", \"once\", \"one\", \"ones\", \"oneself\", \"onto\", \"open\", \"ord\", \"others\", \"otherwise\", \"ought\", \"oughtn\", \"ours\",\n",
    "    \"out\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"pending\",\n",
    "    \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\",\n",
    "    \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provided\", \"provides\", \"providing\",\n",
    "    \"public\", \"put\", \"q\", \"qua\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"real\", \"really\",\n",
    "    \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\",\n",
    "    \"research\", \"respecting\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"round\", \"run\", \"said\", \"sans\",\n",
    "    \"save\", \"saving\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\",\n",
    "    \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\",\n",
    "    \"shall\", \"shalt\", \"shan't\", \"she'd\", \"she'll\", \"shed\", \"shell\", \"shes\", \"short\", \"shouldn\", \"show\", \"showed\", \"shown\",\n",
    "    \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"small\", \"some\",\n",
    "    \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\",\n",
    "    \"sorry\", \"special\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\",\n",
    "    \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"summat\", \"sup\", \"supposing\", \"sure\", \"t's\", \"take\",\n",
    "    \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"that'll\", \"that's\", \"that've\", \"thats\", \"thee\",\n",
    "    \"theirs\", \"themselves\", \"thence\", \"there'll\", \"there's\", \"there've\", \"thereafter\", \"thereby\", \"thered\", \"therefore\",\n",
    "    \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\",\n",
    "    \"theyd\", \"theyre\", \"thine\", \"think\", \"third\", \"tho\", \"thorough\", \"thoroughly\", \"thou\", \"though\", \"thoughh\", \"thousand\",\n",
    "    \"three\", \"thro\", \"throug\", \"throughout\", \"thru\", \"thus\", \"thyself\", \"til\", \"till\", \"tip\", \"today\", \"together\", \"took\",\n",
    "    \"touching\", \"toward\", \"towards\", \"tried\", \"tries\", \"true\", \"truly\", \"try\", \"trying\", \"ts\", \"twas\", \"tween\", \"twere\",\n",
    "    \"twice\", \"twill\", \"twixt\", \"two\", \"twould\", \"u\", \"un\", \"underneath\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\",\n",
    "    \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\",\n",
    "    \"various\", \"versus\", \"via\", \"vice\", \"vis-a-vis\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"wanna\", \"want\", \"wanting\", \"wants\",\n",
    "    \"wasn\", \"wasnt\", \"way\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"wed\", \"welcome\", \"well\", \"went\", \"weren\", \"werent\", \"wert\",\n",
    "    \"what'll\", \"what's\", \"whatever\", \"whats\", \"when's\", \"whence\", \"whencesoever\", \"whenever\", \"where's\", \"whereafter\",\n",
    "    \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whichever\", \"whichsoever\", \"whilst\",\n",
    "    \"whim\", \"whither\", \"who'll\", \"who's\", \"whod\", \"whoever\", \"whole\", \"whomever\", \"whore\", \"whos\", \"whose\", \"whoso\",\n",
    "    \"whosoever\", \"why's\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wonder\", \"wont\", \"words\", \"world\", \"would\",\n",
    "    \"wouldn't\", \"wouldnt\", \"wouldst\", \"www\", \"x\", \"ye\", \"yes\", \"yet\", \"you'd\", \"you're\", \"youd\", \"youre\", \"yourself\", \"z\", \"zero\"\n",
    "]\n",
    "stop_words_extended = list(\n",
    "    set(stop_words_extended + punctuation + list(stopwords.words('english'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "##### Load and clean the 1.6M tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:13:03.009182Z",
     "start_time": "2021-08-18T13:12:56.530220Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('Data/tweets.csv', encoding=\"ISO-8859-1\",\n",
    "                        names=[\"sentiment\", \"tweet_id\", \"created_at\", \"query\", \"username\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:13:05.253326Z",
     "start_time": "2021-08-18T13:13:05.249287Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_date(date):\n",
    "    return datetime.strptime(date.replace(' PDT', ''), \"%a %b %d %H:%M:%S %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:13:57.529782Z",
     "start_time": "2021-08-18T13:13:21.599246Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df['created_at'] = tweets_df['created_at'].apply(convert_date)\n",
    "\n",
    "tweets_df = tweets_df.sort_values(\n",
    "    [\"username\", \"created_at\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:13:59.277089Z",
     "start_time": "2021-08-18T13:13:57.567503Z"
    }
   },
   "outputs": [],
   "source": [
    "user_tweet_counts=tweets_df[['tweet_id', 'username', 'created_at']].groupby(['username']).agg('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:14:02.697416Z",
     "start_time": "2021-08-18T13:14:02.346292Z"
    }
   },
   "outputs": [],
   "source": [
    "users_50 = list(user_tweet_counts['username'][user_tweet_counts['tweet_id']>=50])\n",
    "users_70 = list(user_tweet_counts['username'][user_tweet_counts['tweet_id']>=70])\n",
    "users_100 = list(user_tweet_counts['username'][user_tweet_counts['tweet_id']>=100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:14:04.705906Z",
     "start_time": "2021-08-18T13:14:04.700035Z"
    }
   },
   "outputs": [],
   "source": [
    "def user_tweets_50(username):\n",
    "    if username in users_50:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def user_tweets_70(username):\n",
    "    if username in users_70:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def user_tweets_100(username):\n",
    "    if username in users_100:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def user_tweets_180(username):\n",
    "    if username in users_180:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:15:02.790757Z",
     "start_time": "2021-08-18T13:14:06.856939Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df['_50'] = tweets_df['username'].apply(user_tweets_50)\n",
    "tweets_df['_70'] = tweets_df['username'].apply(user_tweets_70)\n",
    "tweets_df['_100'] = tweets_df['username'].apply(user_tweets_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:15:05.175016Z",
     "start_time": "2021-08-18T13:15:02.857154Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df=tweets_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Pre-process tweets by filtering the text and recording the sentiments of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:17:24.362384Z",
     "start_time": "2021-08-18T13:17:24.117724Z"
    }
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def deEmojify(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               u\"\\U00002500-\\U00002BEF\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def de_abbreviate(token):\n",
    "    \n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow' or token == '2moro':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token in ['hahah', 'hahaha', 'hahahaha']:\n",
    "        return 'haha'\n",
    "    if token == \"mother's\":\n",
    "        return \"mother\"\n",
    "    if token == \"mom's\":\n",
    "        return \"mom\"\n",
    "    if token == \"dad's\":\n",
    "        return \"dad\"\n",
    "    if token == 'bday' or token == 'b-day':\n",
    "        return 'birthday'\n",
    "    if token in [\"i'm\", \"don't\", \"can't\", \"couldn't\", \"aren't\", \"wouldn't\", \"isn't\", \"didn't\", \"hadn't\",\n",
    "                 \"doesn't\", \"won't\", \"haven't\", \"wasn't\", \"hasn't\", \"shouldn't\", \"ain't\", \"they've\"]:\n",
    "        return token.replace(\"'\", \"\")\n",
    "    if token in ['lmao', 'lolz', 'rofl']:\n",
    "        return 'lol'\n",
    "    if token == '<3':\n",
    "        return 'love'\n",
    "    if token == 'thanx' or token == 'thnx':\n",
    "        return 'thanks'\n",
    "    if token == 'goood':\n",
    "        return 'good'\n",
    "    if token in ['amp', 'quot', 'lt', 'gt', '½25', '..', '. .', '. . .', '...']:\n",
    "        return ' '\n",
    "\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "\n",
    "def de_slang(tweet):\n",
    "\n",
    "    tweet = tweet.replace(\"idk\", \"i dont know\")\n",
    "    tweet = tweet.replace(\"i'll\", \"i will\")\n",
    "    tweet = tweet.replace(\"you'll\", \"you will\")\n",
    "    tweet = tweet.replace(\"we'll\", \"we will\")\n",
    "    tweet = tweet.replace(\"it'll\", \"it will\")\n",
    "    tweet = tweet.replace(\"it's\", \"it is\")\n",
    "    tweet = tweet.replace(\"i've\", \"i have\")\n",
    "    tweet = tweet.replace(\"you've\", \"you have\")\n",
    "    tweet = tweet.replace(\"we've\", \"we have\")\n",
    "    tweet = tweet.replace(\"they've\", \"they have\")\n",
    "    tweet = tweet.replace(\"you're\", \"you are\")\n",
    "    tweet = tweet.replace(\"we're\", \"we are\")\n",
    "    tweet = tweet.replace(\"they're\", \"they are\")\n",
    "    tweet = tweet.replace(\"let's\", \"let us\")\n",
    "    tweet = tweet.replace(\"she's\", \"she is\")\n",
    "    tweet = tweet.replace(\"he's\", \"he is\")\n",
    "    tweet = tweet.replace(\"that's\", \"that is\")\n",
    "    tweet = tweet.replace(\"i'd\", \"i would\")\n",
    "    tweet = tweet.replace(\"you'd\", \"you would\")\n",
    "    tweet = tweet.replace(\"there's\", \"there is\")\n",
    "    tweet = tweet.replace(\"what's\", \"what is\")\n",
    "    tweet = tweet.replace(\"how's\", \"how is\")\n",
    "    tweet = tweet.replace(\"who's\", \"who is\")\n",
    "    tweet = tweet.replace(\"y'all\", \"you all\")\n",
    "    tweet = tweet.replace(\"ya'll\", \"you all\")\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_text(tweet):\n",
    "\n",
    "    # replace seeds (as phrases) to unigrams.\n",
    "    for seed in seed_terms_col:\n",
    "        if seed in tweet and \" \" in seed:\n",
    "            tweet = tweet.replace(seed, seed.replace(\" \", \"_\"))\n",
    "\n",
    "    # remove retweet handler\n",
    "    if tweet[:2] == \"RT\":\n",
    "        tweet = tweet[tweet.index(\":\") + 2:]\n",
    "\n",
    "    # remove url from tweet\n",
    "    tweet = re.sub(\n",
    "        r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "\n",
    "    # remove short notations\n",
    "    tweet = de_slang(tweet)\n",
    "\n",
    "    # remove non-ascii characters\n",
    "    tweet = ''.join((filter(lambda x: x in printable, tweet)))\n",
    "\n",
    "    # additional preprocessing\n",
    "    tweet = tweet.replace(\"\\n\", \" \").replace(\" https\", \"\").replace(\"http\", \"\")\n",
    "\n",
    "    # remove all mentions\n",
    "    mentions = re.findall(r\"@\\w+\", tweet)\n",
    "    for mention in mentions:\n",
    "        tweet = tweet.replace(mention, \"\")\n",
    "\n",
    "    # clean usernames and hashtags\n",
    "    for term in re.findall(r\"#\\w+\", tweet):\n",
    "\n",
    "        # remove any punctuations from the hashtag and mention\n",
    "        token = term[1:].translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        segments = ' '.join(segment(token))\n",
    "\n",
    "        tweet = tweet.replace(term, segments)\n",
    "\n",
    "    # remove all punctuations\n",
    "    tweet = re.sub(r\"\"\"\n",
    "               [\"\"\"+\"\".join(punctuation)+\"\"\"]+\n",
    "               \"\"\",\n",
    "                   \" \",\n",
    "                   tweet, flags=re.VERBOSE)\n",
    "\n",
    "    # remove trailing spaces\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r'[\\d-]+', 'NUM', tweet)\n",
    "\n",
    "    # pad NUM with spaces\n",
    "    tweet = tweet.replace(\"NUM\", \" NUM \")\n",
    "\n",
    "    # remove emoticons\n",
    "    tweet = deEmojify(tweet)\n",
    "\n",
    "    # remove all stop words or emojis\n",
    "    tweet = \" \".join([de_abbreviate(word.lower()) for word in tweet_token.tokenize(tweet) if word.lower(\n",
    "    ) not in stop_words_extended and word.lower() not in emojies and len(word) > 1])\n",
    "\n",
    "    # remove multiple spaces\n",
    "    tweet = re.sub(' +', ' ', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess(tweets):\n",
    "\n",
    "    processed_tweets = []\n",
    "\n",
    "    for index, tweet in tqdm.tqdm(tweets.iterrows()):\n",
    "        cleaned_text = preprocess_text(tweet['text'])\n",
    "        sent_score = TextBlob(tweet['text']).sentiment.polarity\n",
    "        vader_compound_score = analyzer.polarity_scores(tweet['text'])[\n",
    "            'compound']\n",
    "        vader_positive_score = analyzer.polarity_scores(tweet['text'])['pos']\n",
    "        vader_negative_score = analyzer.polarity_scores(tweet['text'])['neg']\n",
    "        vader_neutral_score = analyzer.polarity_scores(tweet['text'])['neu']\n",
    "        sent_score_2 = TextBlob(cleaned_text).sentiment.polarity\n",
    "        vader_compound_score_2 = analyzer.polarity_scores(cleaned_text)[\n",
    "            'compound']\n",
    "        vader_positive_score_2 = analyzer.polarity_scores(cleaned_text)['pos']\n",
    "        vader_negative_score_2 = analyzer.polarity_scores(cleaned_text)['neg']\n",
    "        vader_neutral_score_2 = analyzer.polarity_scores(cleaned_text)['neu']\n",
    "\n",
    "        processed_tweets.append([tweet['tweet_id'], tweet['created_at'], tweet['text'], cleaned_text, sent_score, vader_compound_score, vader_positive_score,\n",
    "                                 vader_neutral_score, vader_negative_score, sent_score_2, vader_compound_score_2, vader_positive_score_2, vader_neutral_score_2, vader_negative_score_2])\n",
    "\n",
    "    return pd.DataFrame(processed_tweets, columns=['tweet_id', 'created_at', 'text', 'cleaned_text', 'polarity_raw', 'vader_compound_raw', 'vader_pos_raw',\n",
    "                                                   'vader_neu_raw', 'vader_neg_raw', 'polarity_cleaned', 'vader_compound_cleaned', 'vader_pos_cleaned', 'vader_neu_cleaned', 'vader_neg_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:17:49.465288Z",
     "start_time": "2021-08-18T13:17:34.447089Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6838it [00:14, 456.47it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_tweets = preprocess(tweets_df[[\"tweet_id\", \"created_at\", \"text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Merge the tweets to get the usernames, and filter for tweets count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:18:23.794010Z",
     "start_time": "2021-08-18T13:18:23.493156Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_tweets=pd.merge(preprocessed_tweets, tweets_df[[\"tweet_id\",\"created_at\",\"username\",\"_50\",\"_70\", \"_100\"]], on=[\"tweet_id\",'created_at'])\n",
    "preprocessed_tweets=preprocessed_tweets.drop_duplicates()\n",
    "preprocessed_tweets = preprocessed_tweets.sort_values([\"username\", \"created_at\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:18:30.905560Z",
     "start_time": "2021-08-18T13:18:30.534284Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_tweets.to_csv('Data/tweets_cleaned.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
