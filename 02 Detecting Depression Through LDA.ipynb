{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:27:22.946280Z",
     "start_time": "2021-08-17T22:27:22.254987Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, json, string, datetime, random, itertools\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "from gensim import corpora  #https://radimrehurek.com/gensim/\n",
    "import pandas as pd  #http://pandas.pydata.org/\n",
    "import numpy as np  #http://www.numpy.org/\n",
    "import matplotlib.pyplot as plt  #https://matplotlib.org/\n",
    "import FastLDA\n",
    "from pSSLDA import infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:27:24.345185Z",
     "start_time": "2021-08-17T22:27:24.334068Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"Data/depression_lexicon.json\") as f:\n",
    "    seed_terms = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:44:25.988276Z",
     "start_time": "2021-08-17T22:44:16.284056Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df=pd.read_csv('Data/tweets_cleaned.csv')\n",
    "tweets_df['created_at']=pd.to_datetime(tweets_df['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:44:35.520234Z",
     "start_time": "2021-08-17T22:44:35.483210Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df_50=tweets_df[tweets_df._50==1]\n",
    "tweets_df_70=tweets_df[tweets_df._70==1]\n",
    "tweets_df_100=tweets_df[tweets_df._100==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:44:46.184202Z",
     "start_time": "2021-08-17T22:44:46.029003Z"
    }
   },
   "outputs": [],
   "source": [
    "users=set(tweets_df['username'].to_list())\n",
    "users_50=set(tweets_df_50['username'].to_list())\n",
    "users_70=set(tweets_df_70['username'].to_list())\n",
    "users_100=set(tweets_df_100['username'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:44:56.856438Z",
     "start_time": "2021-08-17T22:44:56.747633Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df=tweets_df[[\"tweet_id\", \"created_at\", \"text\", \"cleaned_text\", \"polarity_raw\",'username']]\n",
    "tweets_df.columns=[\"tweet_ID\", \"created_at\", \"raw_text\", \"cleaned_text\", \"sentiment\",'username']\n",
    "tweets_df_50=tweets_df_50[[\"tweet_id\", \"created_at\", \"text\", \"cleaned_text\", \"polarity_raw\",'username']]\n",
    "tweets_df_50.columns=[\"tweet_ID\", \"created_at\", \"raw_text\", \"cleaned_text\", \"sentiment\",'username']\n",
    "tweets_df_70=tweets_df_70[[\"tweet_id\", \"created_at\", \"text\", \"cleaned_text\", \"polarity_raw\",'username']]\n",
    "tweets_df_70.columns=[\"tweet_ID\", \"created_at\", \"raw_text\", \"cleaned_text\", \"sentiment\",'username']\n",
    "tweets_df_100=tweets_df_100[[\"tweet_id\", \"created_at\", \"text\", \"cleaned_text\", \"polarity_raw\",'username']]\n",
    "tweets_df_100.columns=[\"tweet_ID\", \"created_at\", \"raw_text\", \"cleaned_text\", \"sentiment\",'username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:45:06.967230Z",
     "start_time": "2021-08-17T22:45:06.960125Z"
    }
   },
   "outputs": [],
   "source": [
    "user_sample = random.sample(users_100, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### To emulate PHQ-9 questionare, we bucket tweets based on their creation time with a sliding window of 14 days. Each bucket will then be treated as a document when we run LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:45:25.045664Z",
     "start_time": "2021-08-17T22:45:25.026804Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def create_time_buckets(tweets):\n",
    "    bucketed_tweets = defaultdict(list)\n",
    "    counter=0\n",
    "    for dates in list(chunks(sorted(set(tweets.created_at.dt.date.tolist())), 14)):\n",
    "        mask = (tweets['created_at'].dt.date >= dates[0]) & (tweets['created_at'].dt.date <= dates[-1])\n",
    "        df=tweets.loc[mask]\n",
    "        df=df.drop('username', axis=1)\n",
    "        for index, tweet in df.iterrows():\n",
    "            bucketed_tweets[counter].append(tweet)\n",
    "    return bucketed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:45:34.970950Z",
     "start_time": "2021-08-17T22:45:34.407650Z"
    }
   },
   "outputs": [],
   "source": [
    "user_bucketed_tweets={}\n",
    "for user in user_sample:\n",
    "    user_bucketed_tweets[user] = create_time_buckets(tweets_df[tweets_df['username']==user])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Prepare the data for LDA from the bucketed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:45:50.124896Z",
     "start_time": "2021-08-17T22:45:50.089711Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_prep(bucketed_tweets):\n",
    "\n",
    "    texts = list()\n",
    "\n",
    "    # each bucket is hashed on the start and end date\n",
    "    for bucket in bucketed_tweets:\n",
    "\n",
    "        all_bucket_tweets = \"\"\n",
    "\n",
    "        for tweet in bucketed_tweets[bucket]:\n",
    "\n",
    "            try:\n",
    "                all_bucket_tweets += tweet.cleaned_text + \" \"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        texts.append(all_bucket_tweets.strip().replace(\"\\n\", \"\").split(\" \"))\n",
    "\n",
    "    # assign each word a unique ID\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # remove gaps in id sequence after words that were removed\n",
    "    dictionary.compactify()\n",
    "\n",
    "    voc_size = len(list(dictionary.keys()))\n",
    "\n",
    "    # replace token ids with the token text in each doc and return similar arry of tokens and docs\n",
    "    text_as_ids = list()\n",
    "\n",
    "    # to later be the docvec\n",
    "    doc_as_ids = list()\n",
    "\n",
    "    # number of docs here is the number of buckets\n",
    "    number_of_docs = len(bucketed_tweets)\n",
    "\n",
    "    for x in range(number_of_docs):\n",
    "\n",
    "        doc = texts[x]\n",
    "\n",
    "        for token in doc:\n",
    "            text_as_ids.append(dictionary.token2id[token])\n",
    "            doc_as_ids.append(x)\n",
    "\n",
    "    return text_as_ids, doc_as_ids, voc_size, dictionary.token2id, number_of_docs, bucketed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:46:04.263010Z",
     "start_time": "2021-08-17T22:46:04.218195Z"
    }
   },
   "outputs": [],
   "source": [
    "# docs for us will be here multiple tweets\n",
    "user_LDA_inputs={}\n",
    "for user in user_sample:\n",
    "    user_LDA_inputs[user] = data_prep(user_bucketed_tweets[user])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Run LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:46:18.460504Z",
     "start_time": "2021-08-17T22:46:18.349909Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: topics and signals are used in interchangebly, they both mean the same thing.\n",
    "\n",
    "# calculated the average sentiment of a token based on its occurence in a given set of tweets terms sentiment is therefore taken from the tweet sentiment not targeted sentiment\n",
    "def get_avg_sentiment(bucketed_tweets, token):\n",
    "\n",
    "    term_tweets_sent_scores = get_tweets_by_term(bucketed_tweets, token)\n",
    "\n",
    "    score = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for sent_score in term_tweets_sent_scores:\n",
    "        score += float(sent_score)\n",
    "        count += 1\n",
    "\n",
    "    return score / count\n",
    "\n",
    "\n",
    "def get_tweets_by_term(bucketed_tweets, term):\n",
    "\n",
    "    term_tweets_sent_scores = list()\n",
    "\n",
    "    for bucket in bucketed_tweets:\n",
    "        for tweet in bucketed_tweets[bucket]:\n",
    "            try:\n",
    "                if term in tweet.cleaned_text:\n",
    "                    term_tweets_sent_scores.append(tweet.sentiment)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return term_tweets_sent_scores\n",
    "\n",
    "\n",
    "def get_topics_terms(tup):\n",
    "\n",
    "    estphi = tup[0]\n",
    "    W = tup[1]\n",
    "    T = tup[2]\n",
    "    id2token = tup[3]\n",
    "\n",
    "    # This will contain the mappings of each term to each of our topics\n",
    "    topics_dict = defaultdict(defaultdict)\n",
    "\n",
    "    # find the topic where each term is part of W: vocabulary size\n",
    "    for index in range(W):\n",
    "        # projects one column of the matrix which contains the weight of the term in all of the topics\n",
    "        term_weights = estphi[:, index]\n",
    "\n",
    "        # will contain the largest weight which ->  topic it was assigned to\n",
    "        largest_weight = 0\n",
    "\n",
    "        for weight in term_weights:\n",
    "            if weight > largest_weight:\n",
    "                largest_weight = weight\n",
    "\n",
    "        # this will get the index of the topic with largest weight\n",
    "        term_topic = np.argwhere(term_weights == largest_weight)[0][0]\n",
    "\n",
    "        topics_dict[term_topic][id2token[index]] = largest_weight\n",
    "        \n",
    "    return topics_dict\n",
    "\n",
    "\n",
    "def get_all_terms_sentiments(id2token, w, bucketed_tweets):\n",
    "\n",
    "    seed_term_sentiment = defaultdict(float)\n",
    "\n",
    "    unique_w = list(set(w))\n",
    "\n",
    "    for wi in unique_w:\n",
    "        token = id2token[wi]\n",
    "\n",
    "        if token in seed_terms['signal_1']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_2']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_3']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_4']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_5']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_6']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_7']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_8']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_9']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_10']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "    return seed_term_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:46:34.394857Z",
     "start_time": "2021-08-17T22:46:33.568081Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_LDA(LDA_input, parameters):\n",
    "    \n",
    "\n",
    "    token2id = LDA_input[3]\n",
    "\n",
    "    # number of topics\n",
    "    T = parameters[\"topics_count\"]\n",
    "\n",
    "    (wordvec, docvec, zvec) = ([], [], [])\n",
    "\n",
    "    # vector of words per tweet\n",
    "    wordvec = LDA_input[0]\n",
    "    docvec = LDA_input[1]\n",
    "\n",
    "    # W = vocabulary size\n",
    "    W = LDA_input[2]\n",
    "\n",
    "    (w, d) = (np.array(wordvec, dtype = np.int), np.array(docvec, dtype = np.int))\n",
    "\n",
    "    # Create parameters\n",
    "    alpha = np.ones((1,T)) * 1\n",
    "    beta = np.ones((T,W)) * 0.01\n",
    "\n",
    "    # How many parallel samplers do we wish to use?\n",
    "    P = 1\n",
    "\n",
    "    # Random number seed\n",
    "    randseed =  random.randint(999,999999)\n",
    "\n",
    "    # Number of samples to take\n",
    "    numsamp = 50\n",
    "\n",
    "    # Do parallel inference\n",
    "    final_z = infer(w, d, alpha, beta, numsamp, randseed, P)\n",
    "\n",
    "    # number of documents = tweets\n",
    "    D = LDA_input[4]\n",
    "\n",
    "    # Estimate phi and theta\n",
    "    (nw, nd) = FastLDA.countMatrices(w, W, d, D, final_z, T)\n",
    "    (estphi,esttheta) = FastLDA.estPhiTheta(nw, nd, alpha, beta)\n",
    "\n",
    "    # swap keys with values in the token2id => id2token\n",
    "    id2token = dict((v,k) for k,v in token2id.iteritems())\n",
    "\n",
    "    seed_term_sentiment = get_all_terms_sentiments(id2token, w, LDA_input[5])\n",
    "\n",
    "    # Now, we add z-labels to *force* words into separate topics\n",
    "    \n",
    "    labelweight = 5.0\n",
    "\n",
    "    label0 = np.zeros((T,), dtype=np.float)\n",
    "    label0[0] = labelweight\n",
    "\n",
    "    label1 = np.zeros((T,), dtype=np.float)\n",
    "    label1[1] = labelweight\n",
    "\n",
    "    label2 = np.zeros((T,), dtype=np.float)\n",
    "    label2[2] = labelweight\n",
    "\n",
    "    label3 = np.zeros((T,), dtype=np.float)\n",
    "    label3[3] = labelweight\n",
    "\n",
    "    label4 = np.zeros((T,), dtype=np.float)\n",
    "    label4[4] = labelweight\n",
    "\n",
    "    label5 = np.zeros((T,), dtype=np.float)\n",
    "    label5[5] = labelweight\n",
    "\n",
    "    label6 = np.zeros((T,), dtype=np.float)\n",
    "    label6[6] = labelweight\n",
    "\n",
    "    label7 = np.zeros((T,), dtype=np.float)\n",
    "    label7[7] = labelweight\n",
    "\n",
    "    label8 = np.zeros((T,), dtype=np.float)\n",
    "    label8[8] = labelweight\n",
    "\n",
    "    label9 = np.zeros((T,), dtype=np.float)\n",
    "    label9[9] = labelweight\n",
    "\n",
    "    label10 = np.zeros((T,), dtype=np.float)\n",
    "    label10[10] = labelweight\n",
    "\n",
    "    label11 = np.zeros((T,), dtype=np.float)\n",
    "    label11[11] = labelweight\n",
    "\n",
    "    # signals ids\n",
    "    corpus_signals = [0,1,2,3,4,5,6,7,8,9]\n",
    "   \n",
    "    seed_terms_per_signal = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    z_labels = []\n",
    "    for wi in w:\n",
    "\n",
    "        token = id2token[wi]\n",
    "\n",
    "        # if the word appears in topic 0\n",
    "        if token in seed_terms['signal_1'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label0)\n",
    "\n",
    "            seed_terms_per_signal['signal_1'][token]+=1\n",
    "\n",
    "            if 0 in corpus_signals:\n",
    "                corpus_signals.remove(0)\n",
    "\n",
    "\n",
    "        elif token in seed_terms['signal_2'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label1)\n",
    "\n",
    "            seed_terms_per_signal['signal_2'][token]+=1\n",
    "\n",
    "            if 1 in corpus_signals:\n",
    "                corpus_signals.remove(1)\n",
    "\n",
    "\n",
    "        elif token in seed_terms['signal_3'] and seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label2)\n",
    "\n",
    "            seed_terms_per_signal['signal_3'][token]+=1\n",
    "\n",
    "            if 2 in corpus_signals:\n",
    "                corpus_signals.remove(2)\n",
    "\n",
    "\n",
    "        elif token in seed_terms['signal_4'] and seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label3)\n",
    "            seed_terms_per_signal['signal_4'][token]+=1\n",
    "\n",
    "            if 3 in corpus_signals:\n",
    "                corpus_signals.remove(3)\n",
    "\n",
    "\n",
    "        elif token in seed_terms['signal_5'] and seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label4)\n",
    "\n",
    "            seed_terms_per_signal['signal_5'][token]+=1\n",
    "\n",
    "            if 4 in corpus_signals:\n",
    "                corpus_signals.remove(4)\n",
    "\n",
    "        elif token in seed_terms['signal_6'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label5)\n",
    "\n",
    "            seed_terms_per_signal['signal_6'][token]+=1\n",
    "\n",
    "            if 5 in corpus_signals:\n",
    "                corpus_signals.remove(5)\n",
    "\n",
    "        elif token in seed_terms['signal_7'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label6)\n",
    "\n",
    "            seed_terms_per_signal['signal_7'][token]+=1\n",
    "\n",
    "            if 6 in corpus_signals:\n",
    "                corpus_signals.remove(6)\n",
    "\n",
    "        elif token in seed_terms['signal_8'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label7)\n",
    "\n",
    "            seed_terms_per_signal['signal_8'][token]+=1\n",
    "\n",
    "            if 7 in corpus_signals:\n",
    "                corpus_signals.remove(7)\n",
    "\n",
    "        elif token in seed_terms['signal_9'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label8)\n",
    "\n",
    "            seed_terms_per_signal['signal_9'][token]+=1\n",
    "\n",
    "            if 8 in corpus_signals:\n",
    "                corpus_signals.remove(8)\n",
    "\n",
    "        elif token in seed_terms['signal_10'] and  seed_term_sentiment[token] <= 0:\n",
    "\n",
    "            z_labels.append(label9)\n",
    "\n",
    "            seed_terms_per_signal['signal_10'][token]+=1\n",
    "\n",
    "            if 9 in corpus_signals:\n",
    "                corpus_signals.remove(9)\n",
    "\n",
    "        else:\n",
    "            z_labels.append(None)\n",
    "\n",
    "    # Now inference will find topics with 0 and 1 in separate topics\n",
    "    final_z = infer(w, d, alpha, beta, numsamp, randseed, P, zlabels = z_labels)\n",
    "\n",
    "    # Re-estimate phi and theta\n",
    "    (nw, nd) = FastLDA.countMatrices(w, W, d, D, final_z, T)\n",
    "    (estphi,esttheta) = FastLDA.estPhiTheta(nw, nd, alpha, beta)\n",
    "\n",
    "    # Find the sentiment of each topic cluster based on the tweets where each seed term appered in\n",
    "\n",
    "    tup = (estphi, W, T, id2token)\n",
    "    topics_terms = get_topics_terms(tup)\n",
    "    \n",
    "    sent_scores = defaultdict(list)\n",
    "\n",
    "    counter = 0\n",
    "    for topic in topics_terms:\n",
    "\n",
    "        topic_sent_scores = list()\n",
    "\n",
    "        for term in topics_terms[topic]:\n",
    "            term_tweets_sent_scores = get_tweets_by_term(LDA_input[5], term)\n",
    "\n",
    "            for sent_score in term_tweets_sent_scores:\n",
    "                 topic_sent_scores.append(float(sent_score))\n",
    "\n",
    "        avg = sum(topic_sent_scores) / float(len(topic_sent_scores))\n",
    "\n",
    "        sent_scores[topic] = (topic_sent_scores, avg)\n",
    "        \n",
    "        counter+=1\n",
    "\n",
    "    # post processing of topics. If the bucket has less than 30 tweets then discard the probabilities of that bucket\n",
    "\n",
    "    len_buckets = []\n",
    "    for bucket in LDA_input[5]:\n",
    "        len_b = len(LDA_input[5][bucket])\n",
    "        len_buckets.append(len_b)\n",
    "\n",
    "   \n",
    "    # threshold #1: if number of tweets in that bucket is less than x, then discard that bucket.\n",
    "    min_number_of_tweets_per_bucket = parameters[\"min_tweets_per_bucket\"]\n",
    "    \n",
    "    for x in range(len(len_buckets)):\n",
    "        if len_buckets[x] <= min_number_of_tweets_per_bucket:\n",
    "            esttheta[x, :] = 0\n",
    "\n",
    "    # this will replace zero to the probabilities of the topic by ID if no seed terms were found in the corpus\n",
    "    for topic_id in corpus_signals:\n",
    "        esttheta[:, topic_id] = 0\n",
    "\n",
    "    all_topics_seeds = list()\n",
    "    for signal in seed_terms_per_signal:\n",
    "        all_topics_seeds += seed_terms_per_signal[signal]\n",
    "\n",
    "    # topics to keep\n",
    "    seeds_in_top_k = defaultdict(int)\n",
    "\n",
    "    # number of seed terms that should be in the top topic terms\n",
    "    seeds_threshold = parameters[\"seeds_threshold\"]\n",
    "    \n",
    "    # The number of terms in the topic that we will look into to search for seed terms\n",
    "    top_topic_terms = parameters[\"top_topic_terms\"]\n",
    "\n",
    "    for topic in topics_terms:\n",
    "        for x in range(len(topics_terms[topic])):\n",
    "            term = list(topics_terms[topic])[x]\n",
    "            if x < top_topic_terms:\n",
    "                if term in all_topics_seeds:\n",
    "                    seeds_in_top_k[topic] += 1\n",
    "\n",
    "    # this will replace zero to the probabilities of the topic by ID if no seed terms were found in the corpus\n",
    "    for x in range(len(esttheta[0])):\n",
    "        if x in seeds_in_top_k.keys():\n",
    "            if seeds_in_top_k[x] < seeds_threshold:\n",
    "                esttheta[:, x] = 0\n",
    "        else:\n",
    "            esttheta[:, x] = 0\n",
    "\n",
    "\n",
    "    return (estphi, W, T, id2token), esttheta, topics_terms, seed_terms_per_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T22:46:57.019264Z",
     "start_time": "2021-08-17T22:46:48.330691Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/ipykernel_launcher.py:34: DeprecationWarning: np.loads is deprecated, use pickle.loads instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/ipykernel_launcher.py:196: DeprecationWarning: np.loads is deprecated, use pickle.loads instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n",
      "Sample 48 of 50\n",
      "Sample 49 of 50\n",
      "Online z initialization\n",
      "Assigning documents to partitions\n",
      "Getting indices associated with each partition\n",
      "Create re-numbered doc vectors for each partition\n",
      "Initializing count matrices\n",
      "Launching Sampler processes\n",
      "Computing global nw count matrix\n",
      "Sample 0 of 50\n",
      "Sample 1 of 50\n",
      "Sample 2 of 50\n",
      "Sample 3 of 50\n",
      "Sample 4 of 50\n",
      "Sample 5 of 50\n",
      "Sample 6 of 50\n",
      "Sample 7 of 50\n",
      "Sample 8 of 50\n",
      "Sample 9 of 50\n",
      "Sample 10 of 50\n",
      "Sample 11 of 50\n",
      "Sample 12 of 50\n",
      "Sample 13 of 50\n",
      "Sample 14 of 50\n",
      "Sample 15 of 50\n",
      "Sample 16 of 50\n",
      "Sample 17 of 50\n",
      "Sample 18 of 50\n",
      "Sample 19 of 50\n",
      "Sample 20 of 50\n",
      "Sample 21 of 50\n",
      "Sample 22 of 50\n",
      "Sample 23 of 50\n",
      "Sample 24 of 50\n",
      "Sample 25 of 50\n",
      "Sample 26 of 50\n",
      "Sample 27 of 50\n",
      "Sample 28 of 50\n",
      "Sample 29 of 50\n",
      "Sample 30 of 50\n",
      "Sample 31 of 50\n",
      "Sample 32 of 50\n",
      "Sample 33 of 50\n",
      "Sample 34 of 50\n",
      "Sample 35 of 50\n",
      "Sample 36 of 50\n",
      "Sample 37 of 50\n",
      "Sample 38 of 50\n",
      "Sample 39 of 50\n",
      "Sample 40 of 50\n",
      "Sample 41 of 50\n",
      "Sample 42 of 50\n",
      "Sample 43 of 50\n",
      "Sample 44 of 50\n",
      "Sample 45 of 50\n",
      "Sample 46 of 50\n",
      "Sample 47 of 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 48 of 50\n",
      "Sample 49 of 50\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    topics_count: number of topics/signals to construct using LDA\n",
    "    min_tweets_per_bucket: minimum number of tweets per bucket to constructs and accept a topic from it\n",
    "    seeds_threshold: number of seed terms in the top topic terms\n",
    "    top_topic_terms: the number of terms to consider when searching for seed terms\n",
    "'''\n",
    "\n",
    "parameters = {\"topics_count\": 15, \"min_tweets_per_bucket\": 20, \"seeds_threshold\": 2, \"top_topic_terms\": 25}\n",
    "\n",
    "user_LDA_outputs={}\n",
    "for user in user_sample:\n",
    "    user_LDA_outputs[user] = run_LDA(user_LDA_inputs[user], parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T23:14:04.040443Z",
     "start_time": "2021-08-17T23:14:03.981294Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_depression(LDA_output, user):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        esttheta = LDA_output[1]\n",
    "        \n",
    "        #print (\"Topics Probabilties Over Time\")\n",
    "                \n",
    "        headers = [\"Time Period\", \"Signal-1\", \"Signal-2\", \"Signal-3\", \"Signal-4\", \"Signal-5\",\n",
    "                                  \"Signal-6\", \"Signal-7\", \"Signal-8\", \"Signal-9\", \"Signal-10\"]\n",
    "        \n",
    "        rows = list()\n",
    "        \n",
    "        counter = 0\n",
    "        for key in user_bucketed_tweets[user].keys():\n",
    "            \n",
    "            df = pd.DataFrame(user_bucketed_tweets[user][key])\n",
    "                        \n",
    "            bucket_date = str(df.created_at.min().strftime(\"%d/%m/%Y\")) + \" To \" + \\\n",
    "                          str(df.created_at.max().strftime(\"%d/%m/%Y\"))\n",
    "            \n",
    "            row = [bucket_date] + [esttheta[counter][x] for x in range(len(esttheta[counter])) if x < 10]\n",
    "            \n",
    "            rows.append(row)\n",
    "\n",
    "            # increment counter to get element from the result matrix\n",
    "            counter+=1\n",
    "\n",
    "        topics_probabilities = pd.DataFrame(rows, columns=headers)\n",
    "        \n",
    "        #print (topics_probabilities)\n",
    "        \n",
    "        #topics_probabilities.plot(kind='line')\n",
    "        #plt.show()\n",
    "                \n",
    "        #print (\"Topics Terms\")\n",
    "         \n",
    "\n",
    "        \n",
    "        headers = [\"Topic Number\", \"Topic Terms\"]\n",
    "        rows = list()\n",
    "        \n",
    "        for topic in LDA_output[2]:\n",
    "\n",
    "            topic_nbr = topic+1\n",
    "            \n",
    "            rows.append([topic_nbr, \", \".join(LDA_output[2][topic])])\n",
    "\n",
    "        topics_terms = pd.DataFrame(rows, columns=headers)\n",
    "                    \n",
    "        #print (topics_terms)\n",
    "\n",
    "        #print (\"Seeded Terms Per Topic\")\n",
    "\n",
    "        headers = [\"Topic Number\", \"Seed Terms:Count\"]\n",
    "        rows = list()\n",
    "        \n",
    "        # seed_terms_per_signal\n",
    "        for topic in LDA_output[3]:\n",
    "            \n",
    "            seedTerms = [str(seedTerm)+\":\"+str(LDA_output[3][topic][seedTerm]) \n",
    "                                         for seedTerm in LDA_output[3][topic]]\n",
    "            \n",
    "            rows.append([topic, \", \".join(seedTerms)])\n",
    "        \n",
    "        topics_seeds = pd.DataFrame(rows, columns=headers)\n",
    "                    \n",
    "        #print topics_seeds\n",
    "        return topics_probabilities, topics_terms, topics_seeds\n",
    "    except AssertionError:\n",
    "        print (\"number of tweets is insufficents for depression detection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T23:14:17.249234Z",
     "start_time": "2021-08-17T23:14:17.165088Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_depression_outputs={}\n",
    "for user in user_sample:\n",
    "    user_depression_outputs[user]=detect_depression(user_LDA_outputs[user],user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T23:14:44.795916Z",
     "start_time": "2021-08-17T23:14:44.784574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amysav83', 'Kikirowr', 'MiDesfileNegro', 'thisgoeshere', 'nuttychris']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T23:15:00.071181Z",
     "start_time": "2021-08-17T23:15:00.038432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Signal-1</th>\n",
       "      <th>Signal-2</th>\n",
       "      <th>Signal-3</th>\n",
       "      <th>Signal-4</th>\n",
       "      <th>Signal-5</th>\n",
       "      <th>Signal-6</th>\n",
       "      <th>Signal-7</th>\n",
       "      <th>Signal-8</th>\n",
       "      <th>Signal-9</th>\n",
       "      <th>Signal-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/04/2009 To 25/06/2009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040733</td>\n",
       "      <td>0.0611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13442</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time Period  Signal-1  Signal-2  Signal-3  Signal-4  Signal-5  \\\n",
       "0  07/04/2009 To 25/06/2009       0.0       0.0  0.040733    0.0611       0.0   \n",
       "\n",
       "   Signal-6  Signal-7  Signal-8  Signal-9  Signal-10  \n",
       "0       0.0       0.0       0.0   0.13442        0.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_depression_outputs['amysav83'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T23:15:26.392846Z",
     "start_time": "2021-08-17T23:15:26.356869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic Number</th>\n",
       "      <th>Topic Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>quack, mum, ready, code, goto, shorts, food, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>moz, aaawww, missin, nite, disappointed, twitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>go_to_bed, deserved, thunder, scared, ages, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aaww, ghost, rice, hows, doin, tired, need_to_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>pouring, evening, yas, sore, pic, wk, plane, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>kids, england, hoping, turn, feel, shift, prin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>week, thankies, waaa, yeeey, ohh, oohh, fair, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>weather, afta, appreciating, shit, luv, angry,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>nurses, wil, dead, num, funny, nights, working...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>pills, claim, sooo, service, shame, aww, haha,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>edinburgh, play, move, eh, monday, nxt, ya, du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>lightening, nothiiing, car, fri, nooo, mind, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>wrkin, regret, weight, thur, taught, txt, dont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>cute, head, anythin, wee, bn, coz, fingers, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>workin, tooo, work, upset, betta, soo, cat, fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic Number                                        Topic Terms\n",
       "0              1  quack, mum, ready, code, goto, shorts, food, g...\n",
       "1              2  moz, aaawww, missin, nite, disappointed, twitt...\n",
       "2              3  go_to_bed, deserved, thunder, scared, ages, ye...\n",
       "3              4  aaww, ghost, rice, hows, doin, tired, need_to_...\n",
       "4              5  pouring, evening, yas, sore, pic, wk, plane, m...\n",
       "5              6  kids, england, hoping, turn, feel, shift, prin...\n",
       "6              7  week, thankies, waaa, yeeey, ohh, oohh, fair, ...\n",
       "7              8  weather, afta, appreciating, shit, luv, angry,...\n",
       "8              9  nurses, wil, dead, num, funny, nights, working...\n",
       "9             10  pills, claim, sooo, service, shame, aww, haha,...\n",
       "10            11  edinburgh, play, move, eh, monday, nxt, ya, du...\n",
       "11            12  lightening, nothiiing, car, fri, nooo, mind, h...\n",
       "12            13  wrkin, regret, weight, thur, taught, txt, dont...\n",
       "13            14  cute, head, anythin, wee, bn, coz, fingers, ra...\n",
       "14            15  workin, tooo, work, upset, betta, soo, cat, fo..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_depression_outputs['amysav83'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T23:15:42.069893Z",
     "start_time": "2021-08-17T23:15:42.033930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic Number</th>\n",
       "      <th>Seed Terms:Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>signal_9</td>\n",
       "      <td>dead:1, hurt:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signal_8</td>\n",
       "      <td>angry:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>signal_3</td>\n",
       "      <td>awake:1, need_to_sleep:1, go_to_bed:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>signal_2</td>\n",
       "      <td>disappointed:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>signal_4</td>\n",
       "      <td>tired:3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>signal_10</td>\n",
       "      <td>pills:1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic Number                       Seed Terms:Count\n",
       "0     signal_9                         dead:1, hurt:1\n",
       "1     signal_8                                angry:1\n",
       "2     signal_3  awake:1, need_to_sleep:1, go_to_bed:1\n",
       "3     signal_2                         disappointed:1\n",
       "4     signal_4                                tired:3\n",
       "5    signal_10                                pills:1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_depression_outputs['amysav83'][2]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
