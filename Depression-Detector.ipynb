{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:48:20.256668Z",
     "start_time": "2021-07-19T18:48:12.375089Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, json, string, datetime, random, itertools\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# You should install the following libraries\n",
    "import wordsegment  #https://pypi.python.org/pypi/wordsegment\n",
    "from nltk import TweetTokenizer  #http://www.nltk.org/api/nltk.tokenize.html\n",
    "import tweepy  #https://github.com/tweepy/tweepy\n",
    "from textblob import TextBlob  #https://textblob.readthedocs.io/en/dev/\n",
    "from gensim import corpora  #https://radimrehurek.com/gensim/\n",
    "import pandas as pd  #http://pandas.pydata.org/\n",
    "import numpy as NP  #http://www.numpy.org/\n",
    "import matplotlib.pyplot as plt  #https://matplotlib.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Preparing the depression lexicon to seed the LDA topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:48:28.617443Z",
     "start_time": "2021-07-19T18:48:28.610392Z"
    }
   },
   "outputs": [],
   "source": [
    "# read Depression PHQ-9 Lexicon (DPL) from json file\n",
    "with open(\"depression_lexicon.json\") as f:\n",
    "    seed_terms = json.load(f)\n",
    "\n",
    "# read all seed terms into a list removing the underscore from all seeds\n",
    "all_seeds_raw = [\n",
    "    seed.replace(\"_\", \" \").encode('utf-8') for seed in list(\n",
    "        itertools.chain.from_iterable(\n",
    "            [seed_terms[signal] for signal in seed_terms.keys()]))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Preparing other lexicons and resources to be used in filtering and preprocessing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T18:48:43.721904Z",
     "start_time": "2021-07-19T18:48:43.695876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Other lexicons and resources\n",
    "emojies = [\n",
    "    \":‑)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\",\n",
    "    \":^)\", \":っ)\", \":‑D\", \"8‑D\", \"8D\", \"x‑D\", \"xD\", \"X‑D\", \"XD\", \"=‑D\", \"=D\",\n",
    "    \"=‑3\", \"=3\", \"B^D\", \":-))\", \">:[\", \":‑(\", \":(\", \":‑c\", \":c\", \":‑<\", \":っC\",\n",
    "    \":<\", \":‑[\", \":[\", \":{\", \";(\", \":-||\", \":@\", \">:(\", \":'‑(\", \":'(\", \":'‑)\",\n",
    "    \":')\", \"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D‑':\", \">:O\", \":‑O\",\n",
    "    \":O\", \":‑o\", \":o\", \"8‑0\", \"O_O\", \"o‑o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\", \":*\",\n",
    "    \":-*\", \":^*\", \"(\", \"}{'\", \")\", \";‑)\", \";)\", \"*-)\", \"*)\", \";‑]\", \";]\", \";D\",\n",
    "    \";^)\", \":‑,\", \">:P\", \":‑P\", \":P\", \"X‑P\", \"x‑p\", \"xp\", \"XP\", \":‑p\", \":p\",\n",
    "    \"=p\", \":‑Þ\", \":Þ\", \":þ\", \":‑þ\", \":‑b\", \":b\", \"d:\", \">:\\\\\", \">:/\", \":‑/\",\n",
    "    \":‑.\", \":/\", \":\\\\\", \"=/\", \"=\\\\\", \":L\", \"=L\", \":S\", \">.<\", \":|\", \":‑|\",\n",
    "    \":$\", \":‑X\", \":X\", \":‑#\", \":#\", \"O:‑)\", \"0:‑3\", \"0:3\", \"0:‑)\", \"0:)\",\n",
    "    \"0;^)\", \">:)\", \">;)\", \">:‑)\", \"}:‑)\", \"}:)\", \"3:‑)\", \"3:)\", \"o/\\o\", \"^5\",\n",
    "    \">_>^\", \"^<_<\", \"|;‑)\", \"|‑O\", \":‑J\", \":‑&\", \":&\", \"#‑)\", \"%‑)\", \"%)\",\n",
    "    \":‑###..\", \":###..\", \"<:‑|\", \"<*)))‑{\", \"><(((*>\", \"><>\", \"\\o/\", \"*\\0/*\",\n",
    "    \"@}‑;‑'‑‑‑\", \"@>‑‑>‑‑\", \"~(_8^(I)\", \"5:‑)\", \"~:‑\\\\\", \"//0‑0\\\\\\\\\", \"*<|:‑)\",\n",
    "    \"=:o]\", \"7:^]\", \",:‑)\", \"</3\", \"<3\"\n",
    "]\n",
    "\n",
    "# Tweet tokenizer from NLTK: http://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer\n",
    "nltk_tok = TweetTokenizer(preserve_case=True,\n",
    "                          reduce_len=True,\n",
    "                          strip_handles=True)\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "punctuation.remove(\"-\")\n",
    "punctuation.remove('_')\n",
    "\n",
    "long_stop_list = [\n",
    "    \"a\", \"a's\", \"abaft\", \"able\", \"aboard\", \"about\", \"above\", \"abst\",\n",
    "    \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\",\n",
    "    \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afore\", \"aforesaid\",\n",
    "    \"after\", \"afterwards\", \"again\", \"against\", \"agin\", \"ago\", \"ah\", \"ain't\",\n",
    "    \"aint\", \"albeit\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\",\n",
    "    \"alongside\", \"already\", \"also\", \"although\", \"always\", \"am\", \"american\",\n",
    "    \"amid\", \"amidst\", \"among\", \"amongst\", \"an\", \"and\", \"anent\", \"announce\",\n",
    "    \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\",\n",
    "    \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"apparently\", \"appear\",\n",
    "    \"appreciate\", \"appropriate\", \"approximately\", \"are\", \"aren\", \"aren't\",\n",
    "    \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"aslant\",\n",
    "    \"associated\", \"astride\", \"at\", \"athwart\", \"auth\", \"available\", \"away\",\n",
    "    \"awfully\", \"b\", \"back\", \"bar\", \"barring\", \"be\", \"became\", \"because\",\n",
    "    \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\",\n",
    "    \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\",\n",
    "    \"beneath\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"betwixt\",\n",
    "    \"beyond\", \"biol\", \"both\", \"brief\", \"briefly\", \"but\", \"by\", \"c\", \"c'mon\",\n",
    "    \"c's\", \"ca\", \"came\", \"can\", \"can't\", \"cannot\", \"cant\", \"cause\", \"causes\",\n",
    "    \"certain\", \"certainly\", \"changes\", \"circa\", \"clearly\", \"close\", \"co\",\n",
    "    \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\",\n",
    "    \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"cos\",\n",
    "    \"could\", \"couldn\", \"couldn't\", \"couldnt\", \"couldst\", \"course\", \"currently\",\n",
    "    \"d\", \"dare\", \"dared\", \"daren\", \"dares\", \"daring\", \"date\", \"definitely\",\n",
    "    \"described\", \"despite\", \"did\", \"didn\", \"didn't\", \"different\", \"directly\",\n",
    "    \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"done\", \"dost\",\n",
    "    \"doth\", \"down\", \"downwards\", \"due\", \"during\", \"durst\", \"e\", \"each\",\n",
    "    \"early\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\",\n",
    "    \"elsewhere\", \"em\", \"end\", \"ending\", \"english\", \"enough\", \"entirely\", \"er\",\n",
    "    \"ere\", \"especially\", \"et\", \"et-al\", \"etc\", \"even\", \"ever\", \"every\",\n",
    "    \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\",\n",
    "    \"example\", \"except\", \"excepting\", \"f\", \"failing\", \"far\", \"few\", \"ff\",\n",
    "    \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"for\",\n",
    "    \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"from\", \"further\",\n",
    "    \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\",\n",
    "    \"gives\", \"giving\", \"go\", \"goes\", \"going\", \"gone\", \"gonna\", \"got\", \"gotta\",\n",
    "    \"gotten\", \"greetings\", \"h\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hard\",\n",
    "    \"hardly\", \"has\", \"hasn\", \"hasn't\", \"hast\", \"hath\", \"have\", \"haven\",\n",
    "    \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"hed\", \"hello\", \"help\",\n",
    "    \"hence\", \"her\", \"here\", \"here's\", \"hereafter\", \"hereby\", \"herein\", \"heres\",\n",
    "    \"hereupon\", \"hers\", \"herself\", \"hes\", \"hi\", \"hid\", \"high\", \"him\",\n",
    "    \"himself\", \"his\", \"hither\", \"home\", \"hopefully\", \"how\", \"how's\", \"howbeit\",\n",
    "    \"however\", \"hundred\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"id\", \"ie\", \"if\",\n",
    "    \"ignored\", \"ill\", \"im\", \"immediate\", \"immediately\", \"importance\",\n",
    "    \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\",\n",
    "    \"indicated\", \"indicates\", \"information\", \"inner\", \"inside\", \"insofar\",\n",
    "    \"instantly\", \"instead\", \"into\", \"invention\", \"inward\", \"is\", \"isn\",\n",
    "    \"isn't\", \"it\", \"it'd\", \"it'll\", \"it's\", \"itd\", \"its\", \"itself\", \"j\",\n",
    "    \"just\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\",\n",
    "    \"l\", \"large\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\",\n",
    "    \"least\", \"left\", \"less\", \"lest\", \"let\", \"let's\", \"lets\", \"like\", \"liked\",\n",
    "    \"likely\", \"likewise\", \"line\", \"little\", \"living\", \"ll\", \"long\", \"look\",\n",
    "    \"looking\", \"looks\", \"ltd\", \"m\", \"made\", \"mainly\", \"make\", \"makes\", \"many\",\n",
    "    \"may\", \"maybe\", \"mayn\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\",\n",
    "    \"merely\", \"mg\", \"mid\", \"midst\", \"might\", \"mightn\", \"million\", \"mine\",\n",
    "    \"minus\", \"miss\", \"ml\", \"more\", \"moreover\", \"most\", \"mostly\", \"mr\", \"mrs\",\n",
    "    \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"na\",\n",
    "    \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"neath\", \"necessarily\",\n",
    "    \"necessary\", \"need\", \"needed\", \"needing\", \"needn\", \"needs\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"new\", \"next\", \"nigh\", \"nigher\", \"nighest\",\n",
    "    \"nine\", \"ninety\", \"nisi\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\",\n",
    "    \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\",\n",
    "    \"notwithstanding\", \"novel\", \"now\", \"nowhere\", \"o\", \"obtain\", \"obtained\",\n",
    "    \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\",\n",
    "    \"on\", \"once\", \"one\", \"ones\", \"oneself\", \"only\", \"onto\", \"open\", \"or\",\n",
    "    \"ord\", \"other\", \"others\", \"otherwise\", \"ought\", \"oughtn\", \"our\", \"ours\",\n",
    "    \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"owing\", \"own\", \"p\",\n",
    "    \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"pending\",\n",
    "    \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\",\n",
    "    \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"presumably\",\n",
    "    \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provided\",\n",
    "    \"provides\", \"providing\", \"public\", \"put\", \"q\", \"qua\", \"que\", \"quickly\",\n",
    "    \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"re\", \"readily\", \"real\",\n",
    "    \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\",\n",
    "    \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respecting\",\n",
    "    \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"round\",\n",
    "    \"run\", \"s\", \"said\", \"same\", \"sans\", \"save\", \"saving\", \"saw\", \"say\",\n",
    "    \"saying\", \"says\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\",\n",
    "    \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\",\n",
    "    \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"shalt\",\n",
    "    \"shan\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"shed\", \"shell\",\n",
    "    \"shes\", \"short\", \"should\", \"shouldn\", \"shouldn't\", \"show\", \"showed\",\n",
    "    \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\",\n",
    "    \"similarly\", \"since\", \"six\", \"slightly\", \"small\", \"so\", \"some\", \"somebody\",\n",
    "    \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\",\n",
    "    \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"special\", \"specifically\",\n",
    "    \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\",\n",
    "    \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\",\n",
    "    \"summat\", \"sup\", \"supposing\", \"sure\", \"t\", \"t's\", \"take\", \"taken\",\n",
    "    \"taking\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\",\n",
    "    \"that\", \"that'll\", \"that's\", \"that've\", \"thats\", \"the\", \"thee\", \"their\",\n",
    "    \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"there'll\",\n",
    "    \"there's\", \"there've\", \"thereafter\", \"thereby\", \"thered\", \"therefore\",\n",
    "    \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\",\n",
    "    \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"theyd\", \"theyre\",\n",
    "    \"thine\", \"think\", \"third\", \"this\", \"tho\", \"thorough\", \"thoroughly\",\n",
    "    \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"thro\",\n",
    "    \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"thyself\", \"til\",\n",
    "    \"till\", \"tip\", \"to\", \"today\", \"together\", \"too\", \"took\", \"touching\",\n",
    "    \"toward\", \"towards\", \"tried\", \"tries\", \"true\", \"truly\", \"try\", \"trying\",\n",
    "    \"ts\", \"twas\", \"tween\", \"twere\", \"twice\", \"twill\", \"twixt\", \"two\", \"twould\",\n",
    "    \"u\", \"un\", \"under\", \"underneath\", \"unfortunately\", \"unless\", \"unlike\",\n",
    "    \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"ups\", \"us\", \"use\", \"used\",\n",
    "    \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\",\n",
    "    \"value\", \"various\", \"ve\", \"versus\", \"very\", \"via\", \"vice\", \"vis-a-vis\",\n",
    "    \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"wanna\", \"want\", \"wanting\", \"wants\",\n",
    "    \"was\", \"wasn\", \"wasn't\", \"wasnt\", \"way\", \"we\", \"we'd\", \"we'll\", \"we're\",\n",
    "    \"we've\", \"wed\", \"welcome\", \"well\", \"went\", \"were\", \"weren\", \"weren't\",\n",
    "    \"werent\", \"wert\", \"what\", \"what'll\", \"what's\", \"whatever\", \"whats\", \"when\",\n",
    "    \"when's\", \"whence\", \"whencesoever\", \"whenever\", \"where\", \"where's\",\n",
    "    \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\",\n",
    "    \"wherever\", \"whether\", \"which\", \"whichever\", \"whichsoever\", \"while\",\n",
    "    \"whilst\", \"whim\", \"whither\", \"who\", \"who'll\", \"who's\", \"whod\", \"whoever\",\n",
    "    \"whole\", \"whom\", \"whomever\", \"whore\", \"whos\", \"whose\", \"whoso\",\n",
    "    \"whosoever\", \"why\", \"why's\", \"widely\", \"will\", \"willing\", \"wish\", \"with\",\n",
    "    \"within\", \"without\", \"won't\", \"wonder\", \"wont\", \"words\", \"world\", \"would\",\n",
    "    \"wouldn\", \"wouldn't\", \"wouldnt\", \"wouldst\", \"www\", \"x\", \"y\", \"ye\", \"yes\",\n",
    "    \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"youd\", \"your\",\n",
    "    \"youre\", \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\"\n",
    "]\n",
    "stoplist = long_stop_list + punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "##### Now, to detect depression for a single Twitter user we should crawl all their tweets using the Twitter search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is taken from https://github.com/marado/tweet_dumper\n",
    "def get_all_tweets(screen_name):\n",
    "\n",
    "    #Twitter API credentials - https://apps.twitter.com/\n",
    "    consumer_key = ''\n",
    "    consumer_secret = ''\n",
    "    access_key = ''\n",
    "    access_secret = ''\n",
    "\n",
    "    if (consumer_key == \"\"):\n",
    "        print(\n",
    "            \"You need to set up the script first. Edit it and add your keys.\")\n",
    "        return\n",
    "\n",
    "    # Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\n",
    "    # authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "\n",
    "    # make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name=screen_name, count=200)\n",
    "\n",
    "    # save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    # save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    # keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print \"getting tweets before %s\" % (oldest)\n",
    "\n",
    "        # all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name,\n",
    "                                       count=200,\n",
    "                                       max_id=oldest)\n",
    "\n",
    "        # save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        # update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "    # transform the tweepy tweets into a 2D array that will populate the csv\n",
    "    outtweets = [[tweet.id_str, tweet.created_at,\n",
    "                  tweet.text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "\n",
    "    return outtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the screen name here and run the cell to collect all tweets\n",
    "screen_name = \"\"\n",
    "\n",
    "if screen_name == \"\":\n",
    "    print \"You need to add a screen name first!\"\n",
    "\n",
    "account_tweets = get_all_tweets(screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Now, we should preprocess tweets by filtering the text and recording the sentiments of each tweet\n",
    "\n",
    "Output format: ``` [tweet_ID, created_at, raw_text, cleaned_text, sentiment]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:02:11.493353Z",
     "start_time": "2021-07-19T19:02:11.478319Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(tweet):\n",
    "\n",
    "    # this will replace seeds (as phrases) as unigrams. lack of > lack_of\n",
    "    for seed in all_seeds_raw:\n",
    "        if seed in tweet and \" \" in seed:\n",
    "            tweet = tweet.replace(seed, seed.replace(\" \", \"_\"))\n",
    "\n",
    "    # remove retweet handler\n",
    "    if tweet[:2] == \"RT\":\n",
    "        try:\n",
    "            colon_idx = tweet.index(\":\")\n",
    "            tweet = tweet[colon_idx + 2:]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # remove url from tweet\n",
    "    tweet = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '',\n",
    "                   tweet)\n",
    "\n",
    "    # remove non-ascii characters\n",
    "    tweet = filter(lambda x: x in printable, tweet)\n",
    "\n",
    "    # additional preprocessing\n",
    "    tweet = tweet.replace(\"\\n\", \" \").replace(\" https\", \"\").replace(\"http\", \"\")\n",
    "\n",
    "    # remove all mentions in tweet\n",
    "    mentions = re.findall(r\"@\\w+\", tweet)\n",
    "    for mention in mentions:\n",
    "        tweet = tweet.replace(mention, \"\")\n",
    "\n",
    "    # break usernames and hashtags +++++++++++++\n",
    "    for term in re.findall(r\"#\\w+\", tweet):\n",
    "\n",
    "        token = term[1:]\n",
    "\n",
    "        # remove any punctuations from the hashtag and mention\n",
    "        # ex: Troll_Cinema => TrollCinema\n",
    "        token = token.translate(None, ''.join(string.punctuation))\n",
    "\n",
    "        segments = wordsegment.segment(token)\n",
    "        segments = ' '.join(segments)\n",
    "\n",
    "        tweet = tweet.replace(term, segments)\n",
    "\n",
    "    # remove all punctuations from the tweet text\n",
    "    tweet = \"\".join([char for char in tweet if char not in punctuation])\n",
    "\n",
    "    # remove trailing spaces\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    # remove all tokens in the tweet where the token is\n",
    "    # a stop word or an emoji\n",
    "    tweet = [\n",
    "        word.lower() for word in nltk_tok.tokenize(tweet)\n",
    "        if word.lower() not in stoplist and word.lower() not in emojies\n",
    "        and len(word) > 1\n",
    "    ]\n",
    "\n",
    "    tweet = \" \".join(tweet)\n",
    "\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r'[\\d-]+', 'NUM', tweet)\n",
    "    # padding NUM with spaces\n",
    "    tweet = tweet.replace(\"NUM\", \" NUM \")\n",
    "    # remove multiple spaces in tweet text\n",
    "    tweet = re.sub('\\s{2,}', ' ', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "# input: [Tweet_ID, created_at, text]\n",
    "def preprocess(account_tweets):\n",
    "\n",
    "    preprocessed_tweets = list()\n",
    "\n",
    "    for index, tweet in enumerate(account_tweets):\n",
    "\n",
    "        cleaned_text = preprocess_text(tweet[2])\n",
    "        sent_score = TextBlob(tweet[2].decode(\n",
    "            'ascii', errors=\"ignore\")).sentiment.polarity\n",
    "\n",
    "        # output: [tweet_ID, created_at, raw_text, cleaned_text, sentiment]\n",
    "        preprocessed_tweets.append(\n",
    "            [tweet[0], tweet[1], tweet[2], cleaned_text, sent_score])\n",
    "\n",
    "    return preprocessed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweets = preprocess(account_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Now, to emulate PHQ-9 questionare, we bucket tweets based on their creation time with a sliding window of 14 days. Each bucket will then be treated as a document when we run LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:02:38.459095Z",
     "start_time": "2021-07-19T19:02:38.451083Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_sliding_buckets_on_time(account_tweets):\n",
    "\n",
    "    size_of_bucket = 14  # days\n",
    "\n",
    "    # convert list of lists to pandas dataframe\n",
    "    account_tweets = pd.DataFrame(account_tweets,\n",
    "                                  columns=[\n",
    "                                      \"tweet_ID\", \"created_at\", \"raw_text\",\n",
    "                                      \"cleaned_text\", \"sentiment\"\n",
    "                                  ])\n",
    "\n",
    "    # ensure that Created_at column is of type datetime\n",
    "    account_tweets['created_at'] = pd.to_datetime(account_tweets['created_at'])\n",
    "\n",
    "    min_date = account_tweets.created_at.min()\n",
    "    max_date = account_tweets.created_at.max()\n",
    "    max_date = max_date + datetime.timedelta(days=1)\n",
    "\n",
    "    min_date = min_date.replace(hour=0, minute=0, second=0)\n",
    "    max_date = max_date.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "    new_min = min_date\n",
    "    new_max = min_date + datetime.timedelta(days=size_of_bucket)\n",
    "\n",
    "    # will contain the tweets grouped in buckets\n",
    "    bucketed_tweets = defaultdict(list)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        if new_max <= max_date:\n",
    "\n",
    "            mask = (account_tweets['created_at'] >\n",
    "                    new_min) & (account_tweets['created_at'] <= new_max)\n",
    "\n",
    "            df = account_tweets[mask]\n",
    "\n",
    "            for index, tweet in df.iterrows():\n",
    "\n",
    "                bucketed_tweets[counter].append(tweet)\n",
    "\n",
    "            new_min = new_min + datetime.timedelta(days=1)\n",
    "            new_max = new_max + datetime.timedelta(days=1)\n",
    "            counter += 1\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return bucketed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketed_tweets = build_sliding_buckets_on_time(preprocessed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Prepare the data for LDA from the bucketed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:03:17.466552Z",
     "start_time": "2021-07-19T19:03:17.451516Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_LDA(bucketed_tweets):\n",
    "\n",
    "    texts = list()\n",
    "\n",
    "    # each bucket is hashed on the start and end date\n",
    "    for bucket in bucketed_tweets:\n",
    "\n",
    "        all_bucket_tweets = \"\"\n",
    "\n",
    "        for tweet in bucketed_tweets[bucket]:\n",
    "\n",
    "            try:\n",
    "                all_bucket_tweets += tweet.cleaned_text + \" \"\n",
    "            except:\n",
    "                # some cleaned fields are None. therefore, ignore!\n",
    "                pass\n",
    "\n",
    "        texts.append(all_bucket_tweets.strip().replace(\"\\n\", \"\").split(\" \"))\n",
    "\n",
    "    # assign each word a unique ID\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # remove gaps in id sequence after words that were removed\n",
    "    dictionary.compactify()\n",
    "\n",
    "    voc_size = len(list(dictionary.keys()))\n",
    "\n",
    "    # replace token ids with the token text in each doc and return similar arry of tokens and docs\n",
    "    text_as_ids = list()\n",
    "\n",
    "    # to later be the docvec\n",
    "    doc_as_ids = list()\n",
    "\n",
    "    # number of docs here is the number of buckets\n",
    "    number_of_docs = len(bucketed_tweets)\n",
    "\n",
    "    for x in range(number_of_docs):\n",
    "\n",
    "        doc = texts[x]\n",
    "\n",
    "        for token in doc:\n",
    "            text_as_ids.append(dictionary.token2id[token])\n",
    "            doc_as_ids.append(x)\n",
    "\n",
    "    return text_as_ids, doc_as_ids, voc_size, dictionary.token2id, number_of_docs, bucketed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# docs for us will be here multiple tweets\n",
    "LDA_input = prepare_data_for_LDA(bucketed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Run LDA allowing us to seed the LDA topics using our depression lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:05:43.137944Z",
     "start_time": "2021-07-19T19:05:43.114910Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: topics and signals are used in interchangebly in this code, they both mean the same thing.\n",
    "\n",
    "\n",
    "# calculated the average sentiment of a token based on its occurence in a given set of tweets\n",
    "# terms sentiment is therefore taken from the tweet sentiment not targeted sentiment\n",
    "def get_avg_sentiment(bucketed_tweets, token):\n",
    "\n",
    "    term_tweets_sent_scores = get_tweets_by_term(bucketed_tweets, token)\n",
    "\n",
    "    score = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for sent_score in term_tweets_sent_scores:\n",
    "        score += float(sent_score)\n",
    "        count += 1\n",
    "\n",
    "    return score / count\n",
    "\n",
    "\n",
    "def get_tweets_by_term(bucketed_tweets, term):\n",
    "\n",
    "    term_tweets_sent_scores = list()\n",
    "\n",
    "    for bucket in bucketed_tweets:\n",
    "        for tweet in bucketed_tweets[bucket]:\n",
    "            try:\n",
    "                if term in tweet.cleaned_text:\n",
    "                    term_tweets_sent_scores.append(tweet.sentiment)\n",
    "            except:\n",
    "                # pass on empty text field\n",
    "                pass\n",
    "\n",
    "    return term_tweets_sent_scores\n",
    "\n",
    "\n",
    "def get_topics_terms(tup):\n",
    "\n",
    "    estphi = tup[0]\n",
    "    W = tup[1]\n",
    "    T = tup[2]\n",
    "    id2token = tup[3]\n",
    "\n",
    "    # This will contain the mappings of each term to each of our topics\n",
    "    # topic1 -> termX, termY ...\n",
    "    topics_dict = defaultdict(defaultdict)\n",
    "\n",
    "    print(\"Reading Topics Terms: \")\n",
    "\n",
    "    # find the topic where each term is part of\n",
    "    # W: vocabulary size\n",
    "    for index in range(W):\n",
    "        # projects one column of the matrix which contains the weight of the term in all of the topics\n",
    "        term_weights = estphi[:, index]\n",
    "\n",
    "        # will contain the largest weight which ->  topic it was assigned to\n",
    "        largest_weight = 0\n",
    "\n",
    "        for weight in term_weights:\n",
    "            if weight > largest_weight:\n",
    "                largest_weight = weight\n",
    "\n",
    "        # this will get the index of the topic with largest weight\n",
    "        term_topic = NP.argwhere(term_weights == largest_weight)[0][0]\n",
    "\n",
    "        topics_dict[term_topic][id2token[index]] = largest_weight\n",
    "\n",
    "    print(\"Done Reading Topics Terms\")\n",
    "\n",
    "    return topics_dict\n",
    "\n",
    "\n",
    "def get_all_terms_sentiments(id2token, w, bucketed_tweets):\n",
    "\n",
    "    seed_term_sentiment = defaultdict(float)\n",
    "\n",
    "    unique_w = list(set(w))\n",
    "\n",
    "    for wi in unique_w:\n",
    "        token = id2token[wi]\n",
    "\n",
    "        if token in seed_terms['signal_1']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_2']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_3']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_4']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_5']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_6']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_7']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_8']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_9']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "        elif token in seed_terms['signal_10']:\n",
    "            seed_term_sentiment[token] = get_avg_sentiment(\n",
    "                bucketed_tweets, token)\n",
    "\n",
    "    return seed_term_sentiment"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
